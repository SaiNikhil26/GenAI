{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s226TJEMhDUB"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_openai langsmith langchain_community gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP-qLTaa9wfm"
      },
      "source": [
        "If you are using Jupyter notebook, follow the below instructions. Else skip this step and go to next step\n",
        "\n",
        "**Open .env file in this folder and observe that we have configured OPENAI_API_KEY. Replace it with your own key or key given by me**\n",
        "\n",
        "The Code in the below cell will load the .env file and set environment variables.\n",
        "\n",
        "**Write the code in the below cell and execute it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f6wvVRSc9wfo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFqXNNWejL0y"
      },
      "source": [
        "**Let us see if the model remembers previous messages**\n",
        "\n",
        "If you execute the below code, you will understand that the model will not remember our previous messages by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jJVl7qO2iezM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nice to meet you, Siva! How can I assist you today?\n",
            "I'm sorry, I do not know your name as I am an AI assistant and do not have access to personal information.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import  ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "model = ChatOpenAI()\n",
        "firstresponse = model.invoke([HumanMessage(content=\"Hi! I'm Siva\")])\n",
        "\n",
        "print(firstresponse.content)\n",
        "secondresponse = model.invoke([HumanMessage(content=\"What's my name?\")])\n",
        "\n",
        "print(secondresponse.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, I don't know your name as I am an AI assistant and do not have access to your personal information.\n"
          ]
        }
      ],
      "source": [
        "secondresponse = model.invoke([HumanMessage(content=\"What's my name?\")])\n",
        "\n",
        "print(secondresponse.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScCkjH02i_5z"
      },
      "source": [
        "**Now Let us Pass all the messages manually to the model and observe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wfuUD9s7iqOs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your name is Siva.\n"
          ]
        }
      ],
      "source": [
        "thirdresponse = model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi! I'm Siva\"),\n",
        "        AIMessage(content=\"Hello Siva! How can I assist you today?\"),\n",
        "        HumanMessage(content=\"What's my name?\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(thirdresponse.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGsMBXsljW3b"
      },
      "source": [
        "**Now let us add chatmessagehistory manually**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3jMgtfEdjd9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000294A3C22C00>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. Answer all questions to the best of your ability.'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000294A47DAC90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000294A47E9750>, root_client=<openai.OpenAI object at 0x00000294A47A7550>, root_async_client=<openai.AsyncOpenAI object at 0x00000294A47DAE50>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "chain = prompt | model\n",
        "chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhDR-MLSoGvd"
      },
      "source": [
        "**Now, Let us create ChatMessageHistory , add messages and responses manually**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9dRvDbXPj4g7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='The translation of \"I love programming\" in French is \"J\\'adore programmer.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 39, 'total_tokens': 58, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f56702e7-c848-4067-89a4-d957829c0b9f-0', usage_metadata={'input_tokens': 39, 'output_tokens': 19, 'total_tokens': 58, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "\n",
        "chat_message_history = ChatMessageHistory()\n",
        "\n",
        "chat_message_history.add_user_message(\n",
        "    \"Translate this sentence from English to French: I love programming.\"\n",
        ")\n",
        "response = chain.invoke(\n",
        "    {\"messages\": chat_message_history.messages}\n",
        ")\n",
        "response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTBE8AdE9wfr"
      },
      "source": [
        "**Now, let us add the response from AI into chat message history**\n",
        "\n",
        "**Then add our new usermessage also into chat message history**\n",
        "\n",
        "**If you invoke the chain with all messages in chat message history, you will get response which makes us feel like LLM has memory**\n",
        "\n",
        "**Understand and execute below code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7h5rdaILoBFV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Translate this sentence from English to French: I love programming.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='The translation of \"I love programming\" in French is \"J\\'adore programmer.\"', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What did i ask u?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='You asked me to translate the sentence \"I love programming\" from English to French.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_message_history.add_ai_message(AIMessage(response.content))\n",
        "#chat_message_history.messages\n",
        "\n",
        "chat_message_history.add_user_message(\"What did i ask u?\")\n",
        "\n",
        "finalresponse =  chain.invoke({ \"messages\": chat_message_history.messages })\n",
        "\n",
        "chat_message_history.add_ai_message(AIMessage(finalresponse.content))\n",
        "(chat_message_history.messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xAsriXKosPM"
      },
      "source": [
        "**Now let us use RunnableWithMessageHistory. Let us see how it maintains message history automatically**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAfuZu-TppfO"
      },
      "source": [
        "**Firstly, let us define a function which returns an instance of ChatMessageHistory based on session_id which is default configurable parameter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "r1KRamhno6lM"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igY9UxjTtFwl"
      },
      "source": [
        "**Observe how we are passing get_session_history to RunnableWithMessageHistory Internally, RunnableWithMessageHistory will use get_session_history to get BaseChatMessageHistory for current session**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ipZnzjV9wfs"
      },
      "source": [
        "Understand the below code and execute it in a cell.\n",
        "The below code goes through loop for 2 times asking you to enter your message.\n",
        "\n",
        "For the first time, enter the message \"My name is Siva\"\n",
        "Second time enter the message \"What is my name?\"\n",
        "\n",
        "Once u get response, you will understand that how memory works\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kpkclqn6qLuM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello Nikhil, nice to meet you! How can I assist you today?\n",
            "Your name is Nikhil.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"i am nikhil'\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hello Nikhil, nice to meet you! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 13, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-476d666a-c177-487b-85ee-4fc371badaf0-0', usage_metadata={'input_tokens': 13, 'output_tokens': 17, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Your name is Nikhil.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 42, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-71e923e2-7a25-452b-8e47-144ba34accef-0', usage_metadata={'input_tokens': 42, 'output_tokens': 7, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "count =0\n",
        "config = {\"configurable\": {\"session_id\": \"1122\"}}\n",
        "\n",
        "model_with_message_history = RunnableWithMessageHistory(model, get_session_history)\n",
        "\n",
        "while(count < 2):\n",
        "    content = input(\"Enter your message >> \")\n",
        "    result = model_with_message_history.invoke(\n",
        "        [HumanMessage(content=content)],\n",
        "        config=config,\n",
        "    )\n",
        "    count = count + 1\n",
        "    print(result.content)\n",
        "\n",
        "\n",
        "model_with_message_history.get_session_history(\"1122\").messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46c0oYnq9wfs"
      },
      "source": [
        "# Let us build a chat bot which has memory\n",
        "\n",
        "**Understand the below code and execute it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1NLPckAQKpeR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Dell\\Langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\Dell\\Langchain\\venv\\Lib\\site-packages\\gradio\\chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'configurable': {'session_id': '1122'}}\n",
            "Hello Nikhil! How can I assist you today?\n",
            "[HumanMessage(content=\"i am nikhil'\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Nikhil, nice to meet you! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 13, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-476d666a-c177-487b-85ee-4fc371badaf0-0', usage_metadata={'input_tokens': 13, 'output_tokens': 17, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Nikhil.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 42, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-71e923e2-7a25-452b-8e47-144ba34accef-0', usage_metadata={'input_tokens': 42, 'output_tokens': 7, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='hello, this is nikhil', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Nikhil! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 63, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-378f3f63-644e-416c-9e5a-b4548b5bd0c8-0', usage_metadata={'input_tokens': 63, 'output_tokens': 12, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "{'configurable': {'session_id': '1122'}}\n",
            "Your name is Nikhil.\n",
            "[HumanMessage(content=\"i am nikhil'\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Nikhil, nice to meet you! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 13, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-476d666a-c177-487b-85ee-4fc371badaf0-0', usage_metadata={'input_tokens': 13, 'output_tokens': 17, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Nikhil.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 42, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-71e923e2-7a25-452b-8e47-144ba34accef-0', usage_metadata={'input_tokens': 42, 'output_tokens': 7, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='hello, this is nikhil', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Nikhil! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 63, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-378f3f63-644e-416c-9e5a-b4548b5bd0c8-0', usage_metadata={'input_tokens': 63, 'output_tokens': 12, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Nikhil.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 87, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-eb255ebf-6d61-4964-9080-37dd1ce3f3c3-0', usage_metadata={'input_tokens': 87, 'output_tokens': 7, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "{'configurable': {'session_id': '1122'}}\n",
            "You're welcome! If you have any more questions, feel free to ask.\n",
            "[HumanMessage(content=\"i am nikhil'\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Nikhil, nice to meet you! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 13, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-476d666a-c177-487b-85ee-4fc371badaf0-0', usage_metadata={'input_tokens': 13, 'output_tokens': 17, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Nikhil.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 42, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-71e923e2-7a25-452b-8e47-144ba34accef-0', usage_metadata={'input_tokens': 42, 'output_tokens': 7, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='hello, this is nikhil', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Nikhil! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 63, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-378f3f63-644e-416c-9e5a-b4548b5bd0c8-0', usage_metadata={'input_tokens': 63, 'output_tokens': 12, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Nikhil.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 87, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-eb255ebf-6d61-4964-9080-37dd1ce3f3c3-0', usage_metadata={'input_tokens': 87, 'output_tokens': 7, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You're welcome! If you have any more questions, feel free to ask.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 102, 'total_tokens': 119, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-859f5e61-d0bb-418b-b6d4-c755a9cbd6ad-0', usage_metadata={'input_tokens': 102, 'output_tokens': 17, 'total_tokens': 119, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "#session_id = input(\"Enter a session id >> \")\n",
        "\n",
        "def predict(message, history):\n",
        "   config = {\"configurable\": {\"session_id\": \"1122\"}}\n",
        "   print(config)\n",
        "   model_with_message_history = RunnableWithMessageHistory(model, get_session_history)\n",
        "\n",
        "   response1= model_with_message_history.invoke(\n",
        "        [HumanMessage(content=message)],\n",
        "        config=config,\n",
        "    ).content\n",
        "   print(response1)\n",
        "\n",
        "   print(model_with_message_history.get_session_history(\"1122\").messages)\n",
        "   return response1\n",
        "\n",
        "\n",
        "\n",
        "gr.ChatInterface(predict).launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdDNGd4ytnMO"
      },
      "source": [
        "**Now Let us use ChatPromptTemplate with RunnableWithMessageHistory**\n",
        "\n",
        "Define a prompt using ChatPromptTemplate and then create a chain using it  as shown below\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gaM4BqiQtxyO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['language', 'mychat_history', 'myinput'], input_types={'mychat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000294A3C22C00>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a helpful assistant. Answer all questions to the best of your ability in {language}.'), additional_kwargs={}), MessagesPlaceholder(variable_name='mychat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['myinput'], input_types={}, partial_variables={}, template='{myinput}'), additional_kwargs={})])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000294A47DAC90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000294A47E9750>, root_client=<openai.OpenAI object at 0x00000294A47A7550>, root_async_client=<openai.AsyncOpenAI object at 0x00000294A47DAE50>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [  (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
        "        ),  MessagesPlaceholder(variable_name=\"mychat_history\"),\n",
        "        (\"human\" ,\"{myinput}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain =  prompt | model\n",
        "chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NchqHeey9wfs"
      },
      "source": [
        "**Observe how we have Created  RunnableWithMessageHistory using the above chain. Also Observe input_messages_key and  history_messages_key**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Em9iBYNQuAJW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
              "  mychat_history: RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
              "}), kwargs={}, config={'run_name': 'insert_history'}, config_factories=[])\n",
              "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x00000294A603B380>, input_messages_key='myinput', history_messages_key='mychat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_with_message_history = RunnableWithMessageHistory(\n",
        "    chain, get_session_history, input_messages_key=\"myinput\", history_messages_key=\"mychat_history\"\n",
        ")\n",
        "model_with_message_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sevpsfQ9wft"
      },
      "source": [
        "**Understand how we are creating a chatbot using the below code and execute it in a cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NEOfBqJ9hWkD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Dell\\Langchain\\venv\\Lib\\site-packages\\gradio\\chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'configurable': {'session_id': '1298'}}\n",
            "नमस्ते, Nikhil! आपकी सेवा में कैसे मदद कर सकते हैं?\n",
            "[HumanMessage(content='hi this is nikhil', additional_kwargs={}, response_metadata={}), AIMessage(content='你好，Nikhil！有什么可以帮到你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 35, 'total_tokens': 56, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c5964fd5-11c5-4d8d-bb32-57d6b78d662a-0', usage_metadata={'input_tokens': 35, 'output_tokens': 21, 'total_tokens': 56, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='hello this is nikhil', additional_kwargs={}, response_metadata={}), AIMessage(content='नमस्ते, Nikhil! आपकी सेवा में कैसे मदद कर सकते हैं?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 69, 'total_tokens': 113, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3bc55d4-09ef-4b6d-8683-3f7e51cb634d-0', usage_metadata={'input_tokens': 69, 'output_tokens': 44, 'total_tokens': 113, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "{'configurable': {'session_id': '1298'}}\n",
            "आपका नाम निखिल है।\n",
            "[HumanMessage(content='hi this is nikhil', additional_kwargs={}, response_metadata={}), AIMessage(content='你好，Nikhil！有什么可以帮到你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 35, 'total_tokens': 56, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c5964fd5-11c5-4d8d-bb32-57d6b78d662a-0', usage_metadata={'input_tokens': 35, 'output_tokens': 21, 'total_tokens': 56, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='hello this is nikhil', additional_kwargs={}, response_metadata={}), AIMessage(content='नमस्ते, Nikhil! आपकी सेवा में कैसे मदद कर सकते हैं?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 69, 'total_tokens': 113, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3bc55d4-09ef-4b6d-8683-3f7e51cb634d-0', usage_metadata={'input_tokens': 69, 'output_tokens': 44, 'total_tokens': 113, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='आपका नाम निखिल है।', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 125, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-cf9cc130-000b-40a1-b443-60db89f71329-0', usage_metadata={'input_tokens': 125, 'output_tokens': 21, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def predict(message, history):\n",
        "   config = {\"configurable\": {\"session_id\": \"1298\"}}\n",
        "   print(config)\n",
        "\n",
        "\n",
        "   response1= model_with_message_history.invoke(\n",
        "        {\"myinput\": message,\"language\" : \"Hindi\"},\n",
        "        config=config,\n",
        "    ).content\n",
        "   print(response1)\n",
        "\n",
        "   print(model_with_message_history.get_session_history(\"1298\").messages)\n",
        "   return response1\n",
        "\n",
        "\n",
        "\n",
        "gr.ChatInterface(predict).launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYq-E1nTwuzj"
      },
      "source": [
        "**Let us see how to stream the result**\n",
        "\n",
        "**Execute the below code in   cell and understand it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGGdkIqBwFT0"
      },
      "outputs": [],
      "source": [
        "content = input(\">> \")\n",
        "\n",
        "streamtresult = model_with_message_history.stream(\n",
        "       {\"input\": content,\"language\" : \"English\"},\n",
        "        config=config,\n",
        "    )\n",
        "for token in streamtresult:\n",
        "  print(token.content,end='|')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQxxi7AHxAez"
      },
      "source": [
        "**Let is understand how to customize**\n",
        "\n",
        "get_session_history functioned defined below uses a combination of user_id and conversation_id\n",
        "\n",
        "Observe how we are customizing  RunnableWithMessageHistory using ConfigurableFieldSpec\n",
        "\n",
        "**Execute the below cell and understand it**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jD7nfzuqxE1U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "नमस्ते निखिल, आपका स्वागत है। आपको कैसे मदद मिल सकती है?\n",
            "आपका नाम निखिल है।\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='hi this is nikhil', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='नमस्ते निखिल, आपका स्वागत है। आपको कैसे मदद मिल सकती है?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 35, 'total_tokens': 91, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-cec89db8-a905-4f48-9f97-65b4eb0cf075-0', usage_metadata={'input_tokens': 35, 'output_tokens': 56, 'total_tokens': 91, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='आपका नाम निखिल है।', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 103, 'total_tokens': 124, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-eeb306f5-d459-4fdd-af28-b2e69fe84eaa-0', usage_metadata={'input_tokens': 103, 'output_tokens': 21, 'total_tokens': 124, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
        "    if (user_id, conversation_id) not in store:\n",
        "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
        "    return store[(user_id, conversation_id)]\n",
        "\n",
        "\n",
        "chain = prompt | model\n",
        "model_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"myinput\", history_messages_key=\"mychat_history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"user_id\", annotation=str, name=\"User ID\",is_shared=True,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"conversation_id\", annotation=str, name=\"Conversation ID\",is_shared=True,\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
        "count =0\n",
        "while(count < 2):\n",
        "    content = input(\">> \")\n",
        "    result = model_with_message_history.invoke(\n",
        "       {\"myinput\": content,\"language\" : \"Hindi\"},\n",
        "        config=config,\n",
        "    )\n",
        "    count = count + 1\n",
        "\n",
        "    print(result.content)\n",
        "model_with_message_history.get_session_history(\"123\",\"1\").messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQHOd-b0yg80"
      },
      "source": [
        "**Using FileChatMessageHistory to persist messages**\n",
        "\n",
        "**Execute the below code to understand how chatmessage history is saved to a file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "नमस्ते जॉन, आपकी सेवा में। आपको कैसे मदद मिल सकती है?\n",
            "आपका नाम जॉन है।\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='hi this is john', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='नमस्ते जॉन, आपकी सेवा में। आपको कैसे मदद मिल सकती है?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 33, 'total_tokens': 86, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-da01d3cc-7f89-48a3-85bc-1bcc806b3738-0', usage_metadata={'input_tokens': 33, 'output_tokens': 53, 'total_tokens': 86, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='आपका नाम जॉन है।', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 98, 'total_tokens': 118, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-65baaaf3-551f-4264-9eb9-a5b3ea354864-0', usage_metadata={'input_tokens': 98, 'output_tokens': 20, 'total_tokens': 118, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='hi this is john', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='नमस्ते जॉन, आपकी सेवा में। आपको कैसे मदद मिल सकती है?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 129, 'total_tokens': 182, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3f911df-eda7-4b8d-9cb8-e6017ab92c19-0', usage_metadata={'input_tokens': 129, 'output_tokens': 53, 'total_tokens': 182, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='what is my name>', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='आपका नाम जॉन है।', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 194, 'total_tokens': 214, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2ead3523-4c52-41c6-a6bb-ed9888bd25ab-0', usage_metadata={'input_tokens': 194, 'output_tokens': 20, 'total_tokens': 214, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "from langchain_community.chat_message_histories import FileChatMessageHistory\n",
        "\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
        "    if (user_id, conversation_id) not in store:\n",
        "        store[(user_id, conversation_id)] = FileChatMessageHistory(\n",
        "            user_id + conversation_id + \"savedconversationmessages.json\"\n",
        "        )\n",
        "    return store[(user_id, conversation_id)]\n",
        "\n",
        "\n",
        "chain = prompt | model\n",
        "model_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"myinput\",\n",
        "    history_messages_key=\"mychat_history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"user_id\",\n",
        "            annotation=str,\n",
        "            name=\"User ID\",\n",
        "            is_shared=True,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"conversation_id\",\n",
        "            annotation=str,\n",
        "            name=\"Conversation ID\",\n",
        "            is_shared=True,\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
        "count = 0\n",
        "while count < 2:\n",
        "    content = input(\">> \")\n",
        "    result = model_with_message_history.invoke(\n",
        "        {\"myinput\": content, \"language\": \"Hindi\"},\n",
        "        config=config,\n",
        "    )\n",
        "    count = count + 1\n",
        "\n",
        "    print(result.content)\n",
        "model_with_message_history.get_session_history(\"123\", \"1\").messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('123',\n",
              "  '1'): <langchain_community.chat_message_histories.file.FileChatMessageHistory at 0x294b8bdc790>}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "store"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
