{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatOpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the combined chain using LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Sure! Here are three lawyer jokes for you:\\n\\n1. Why don’t lawyers go to the beach?  \\n   Because cats keep trying to bury them in the sand!\\n\\n2. What’s the difference between a lawyer and a herd of buffalo?  \\n   The lawyer charges more!\\n\\n3. How many lawyer jokes are there, anyway?  \\n   Only three. The rest are true stories! \\n\\nHope you got a good chuckle out of those!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 26, 'total_tokens': 117, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-ef7fffed-16a8-4b4e-b585-08d5314c675b-0' usage_metadata={'input_tokens': 26, 'output_tokens': 91, 'total_tokens': 117, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | model\n",
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StrOutputParser**: OutputParser that parses LLMResult into the top likely string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are three lawyer-themed jokes for you:\n",
      "\n",
      "1. Why don’t lawyers go to the beach?  \n",
      "   Because cats keep trying to bury them in the sand!\n",
      "\n",
      "2. What’s the difference between a lawyer and a herd of buffalo?  \n",
      "   The lawyer charges more!\n",
      "\n",
      "3. How can you tell when a lawyer is lying?  \n",
      "   Their lips are moving—just like when they’re talking about their fees! \n",
      "\n",
      "Hope you enjoy them!\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | model | StrOutputParser()\n",
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ChatOpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create individual runnables (steps in a chain)\n",
    "\n",
    "- Runnable: Is a task\n",
    "- RunnableLambda: Is a task that is a lambda function\n",
    "- RunnableSequence: Is a sequence of tasks\n",
    "\n",
    "All those tasks put together in a sequence is called **Runnable Seqeunce** or **Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
    "invoke_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
    "parse_output = RunnableLambda(lambda x: x.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableSequence (equivalent to the LCEL chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* first: single runnable\n",
    "* middle: multiple runnables (pass in as a list)\n",
    "* third: single runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableSequence(first = format_prompt,middle = [invoke_model], last=parse_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are three lawyer-themed jokes for you:\n",
      "\n",
      "1. Why don’t lawyers go to the beach?\n",
      "   Because cats keep trying to bury them in the sand!\n",
      "\n",
      "2. What's the difference between a lawyer and a herd of buffalo?\n",
      "   The lawyer charges more!\n",
      "\n",
      "3. How can you tell if a lawyer is lying?\n",
      "   Their lips are moving!\n",
      "\n",
      "Hope these gave you a laugh!\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains Extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ChatOpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a comedian who tells jokes about {topic}\"),\n",
    "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional processing steps using RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
    "count_words = RunnableLambda(lambda x: f\"Word count:{len(x.split())}\\n{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined chain using LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count:65\n",
      "SURE, HERE ARE THREE LAWYER-THEMED JOKES FOR YOU:\n",
      "\n",
      "1. WHY DON'T LAWYERS PLAY HIDE AND SEEK?  \n",
      "   BECAUSE GOOD LUCK HIDING WHEN EVERY TIME THEY FIND YOU, THEY’LL JUST BILL YOU BY THE HOUR!\n",
      "\n",
      "2. WHAT'S THE DIFFERENCE BETWEEN A LAWYER AND A HERD OF BUFFALO?  \n",
      "   THE LAWYER CHARGES MORE!\n",
      "\n",
      "3. WHY DID THE LAWYER CROSS THE ROAD?  \n",
      "   TO SUE THE CHICKEN ON THE OTHER SIDE!\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | model | StrOutputParser()| uppercase_output | count_words\n",
    "\n",
    "response = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ChatOpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are an expert product reviewer\"),\n",
    "    (\"human\",\"List the main features of the product {product_name}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pro analysis step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pros(features):\n",
    "    pros_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"You are an expert product reviewer\"),\n",
    "        (\"human\",\"Given these features : {features}, list the pros of these features\")\n",
    "    ])\n",
    "    return pros_template.format_prompt(features=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define cons analysis step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cons(features):\n",
    "    cons_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert product reviewer\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given these features : {features}, list the cons of these features\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return cons_template.format_prompt(features=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Pros and Cons into a final review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pros_cons(pros, cons):\n",
    "    return f\"Pros: {pros}\\nCons: {cons}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify branches with Langchain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pros_branch_chain = RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
    "cons_branch_chain = RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined chain using LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Runnable that runs a mapping of Runnables in parallel, and returns a mapping of their outputs.\n",
    "- RunnableParallel is one of the two main composition primitives for the LCEL, alongside RunnableSequence. It invokes Runnables concurrently, providing the same input to each\n",
    "- For each key-value pair in the RunnableParallel branches, a runnable is invoked and executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    prompt_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
    "    | RunnableLambda(\n",
    "        lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pros: The MacBook Pro, as described with its features, has numerous advantages that cater to professionals and power users alike. Here are the pros associated with each of those features:\n",
      "\n",
      "1. **Display**:\n",
      "   - **Stunning Visuals**: The Retina display with True Tone technology enhances color accuracy and brightness, making it ideal for graphic design, photo editing, and content consumption.\n",
      "   - **Exceptional Brightness**: Up to 1600 nits allows for excellent visibility in bright environments and boosts HDR content.\n",
      "   - **Wide Color Gamut**: P3 color support ensures true-to-life colors, enhancing creative work.\n",
      "   - **Smooth Scrolling**: ProMotion technology with a 120Hz refresh rate delivers an ultra-smooth experience, making tasks like scrolling and gaming more engaging.\n",
      "\n",
      "2. **Chip**:\n",
      "   - **Powerful Performance**: Apple Silicon (M1 Pro, M1 Max, M2) provides robust multi-core performance for demanding applications and workloads.\n",
      "   - **Efficient Processing**: High-performance and efficiency cores contribute to optimal performance without significant power draw, enhancing battery life.\n",
      "   - **Integrated Graphics**: A high-core GPU allows for advanced graphics capabilities, appealing to gamers and professional creatives alike.\n",
      "\n",
      "3. **Battery Life**:\n",
      "   - **All-Day Use**: Up to 17-20 hours of battery life ensures productivity without constantly needing to recharge.\n",
      "   - **Convenient Fast Charging**: Reduces downtime, allowing for quick top-ups during busy workdays.\n",
      "\n",
      "4. **Memory and Storage**:\n",
      "   - **Unified Memory Architecture**: Offers fast access to data, which is crucial for performance-intensive applications.\n",
      "   - **Expansive Storage Options**: Up to 8TB SSD provides ample room for files, apps, and large media projects.\n",
      "\n",
      "5. **Connectivity**:\n",
      "   - **Versatile Ports**: Thunderbolt ports allow for high-speed data transfer, daisy-chaining multiple devices, and fast charging, meeting diverse professional needs.\n",
      "   - **External Display Support**: HDMI port makes it easy to connect to monitors, enhancing productivity and making presentations simpler.\n",
      "   - **Media Transfers**: The SDXC card slot simplifies transferring photos and videos directly from cameras.\n",
      "   - **Fast Wireless Connectivity**: Wi-Fi 6 and Bluetooth 5.0 ensure fast, reliable connections to the internet and peripherals.\n",
      "\n",
      "6. **Audio and Video**:\n",
      "   - **Immersive Sound**: High-fidelity six-speaker system delivers rich audio, ideal for music production and media consumption.\n",
      "   - **Improved Communication**: Studio-quality microphones enhance voice clarity for video calls and recordings.\n",
      "   - **High-Quality Video Calls**: The 1080p FaceTime HD camera ensures clearer video quality, essential for remote work and virtual meetings.\n",
      "\n",
      "7. **Keyboard and Trackpad**:\n",
      "   - **Comfortable Typing**: Magic Keyboard design enhances typing comfort over long periods, which is beneficial for extensive writing and programming tasks.\n",
      "   - **Precision Control**: The large Force Touch trackpad allows for accurate and responsive cursor control, making design work smoother.\n",
      "\n",
      "8. **Operating System**:\n",
      "   - **Productivity and Creativity**: macOS is designed for robust performance and reliability, featuring numerous productivity and creative applications that cater to various professional needs.\n",
      "\n",
      "9. **Design**:\n",
      "   - **Premium Build Quality**: The durable aluminum chassis looks sleek and feels professional, appealing to target users.\n",
      "   - **Color Variations**: Having multiple color options allows users to choose a style that fits their personal aesthetic.\n",
      "\n",
      "10. **Security**:\n",
      "    - **Secure Authentication**: Touch ID provides quick and secure access, enhancing user privacy.\n",
      "    - **Enhanced Security Features**: Integration of security chips within Apple Silicon improves system security against malware and threats.\n",
      "\n",
      "In summary, the MacBook Pro's features support high performance, outstanding visual and audio quality, robust connectivity, and a secure user experience, making it a compelling choice for professionals across various fields.\n",
      "Cons: While the MacBook Pro offers an impressive array of features, there are some potential downsides and cons associated with its design and functionality. Here’s a list of the cons for the features mentioned:\n",
      "\n",
      "1. **Display**:\n",
      "   - **High price point**: The advanced display features, such as high brightness and ProMotion technology, contribute to a higher overall cost of the device.\n",
      "   - **Reflection and glare**: The glossy finish, while vibrant, can lead to reflections and glare in bright environments.\n",
      "\n",
      "2. **Chip**:\n",
      "   - **Limited upgradeability**: The unified memory and storage are soldered to the motherboard, meaning users cannot upgrade these components after purchase.\n",
      "   - **Compatibility issues**: Some professional software, particularly legacy applications or specific industry-standard software, may not be optimized for Apple Silicon chips.\n",
      "\n",
      "3. **Battery Life**:\n",
      "   - **Variable performance**: While the battery life can be outstanding, heavy tasks like gaming or high-resolution video editing can significantly reduce battery life.\n",
      "   - **Fast charging requirements**: Fast charging may require specific chargers to achieve optimal speeds, potentially incurring additional costs.\n",
      "\n",
      "4. **Memory and Storage**:\n",
      "   - **Costly upgrades**: Higher RAM and storage options can add to the overall cost, making it expensive to configure the laptop for heavy use.\n",
      "   - **No microSD expansion**: Users unable to afford larger built-in storage options might find the lack of expandable storage limiting.\n",
      "\n",
      "5. **Connectivity**:\n",
      "   - **Limited port selection**: While Thunderbolt ports are versatile, the lack of USB-A or other non-Thunderbolt ports can require additional adapters and dongles.\n",
      "   - **Peripherals compatibility**: Users who rely on older peripherals may need adapters, adding extra hassle.\n",
      "\n",
      "6. **Audio and Video**:\n",
      "   - **Camera quality**: Despite being 1080p, the FaceTime HD camera may still fall short of the expectations of users accustomed to higher-end webcams, especially in low-light conditions.\n",
      "   - **Speakers handling**: The high-fidelity six-speaker system may not meet the expectations of audiophiles compared to dedicated audio solutions.\n",
      "\n",
      "7. **Keyboard and Trackpad**:\n",
      "   - **Keyboard preferences**: Some users may prefer different keyboard layouts or feel, and the Magic Keyboard may not suit everyone’s typing style.\n",
      "   - **Trackpad size**: While the larger trackpad is great for gestures, it can be accidentally activated when users are typing, leading to unintentional cursor movements.\n",
      "\n",
      "8. **Operating System**:\n",
      "   - **macOS limitations**: Some users may find macOS limiting compared to other operating systems, especially those who rely on certain software that is Windows-exclusive.\n",
      "   - **Learning curve**: Users familiar with other OS environments (like Windows) may take time to adapt to macOS functionalities.\n",
      "\n",
      "9. **Design**:\n",
      "   - **Weight**: The aluminum chassis, while strong, can make the MacBook Pro heavier than some competing ultra-portable laptops, making it less ideal for those prioritizing portability.\n",
      "   - **Heat output**: Under heavy workloads, the laptop may run hot to the touch, potentially affecting user comfort during extended use.\n",
      "\n",
      "10. **Security**:\n",
      "    - **Touch ID inaccuracies**: Touch ID can sometimes misread fingerprints, leading to frustrations with authentication.\n",
      "    - **Potential reliance on Apple ecosystem**: The strong security features might tie users more deeply into the Apple ecosystem, making it harder to use non-Apple products without compromise.\n",
      "\n",
      "While the MacBook Pro stands out as a top-tier laptop for professionals, weighing these cons against the benefits is crucial for potential buyers to determine if it meets their specific needs.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ChatOpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt templates for different types of feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    (\"human\",\"Generate a thank you note for this positive feedback: {feedback}\")\n",
    "])\n",
    "\n",
    "negative_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    (\"human\",\"Generate a response addressing this negative feedback: {feedback}\")\n",
    "])\n",
    "\n",
    "neutral_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    (\"human\",\"Generate a request for more details for this neutral feedback: {feedback}\")\n",
    "])\n",
    "\n",
    "escalate_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    (\"human\",\"Generate a message to escalate this to feedback to human agent: {feedback}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback Classification Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    (\"human\",\"Classify this feedback as positive,negative,neutral or escalate: {feedback}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Runnable branches for handling feedbacks**\n",
    "\n",
    "- Runnable that selects which branch to run based on a condition.\n",
    "\n",
    "- The Runnable is initialized with a list of (condition, Runnable) pairs anda default branch.\n",
    "\n",
    "- When operating on an input, the first condition that evaluates to True is selected, and the corresponding Runnable is run on the input.\n",
    "\n",
    "- If no condition evaluates to True, the default branch is run on the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches = RunnableBranch(\n",
    "    (\n",
    "        lambda x: \"positive\" in x,\n",
    "        positive_feedback_template | model | StrOutputParser()\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"negative\" in x,\n",
    "        negative_feedback_template | model | StrOutputParser()\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"neutral\" in x,\n",
    "        neutral_feedback_template | model | StrOutputParser()\n",
    "    ),\n",
    "    escalate_feedback_template | model | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_chain = classification_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine classification chain and response generation into one chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = classification_chain | branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Response to Negative Feedback:**\n",
      "\n",
      "Dear [Name],\n",
      "\n",
      "Thank you for taking the time to share your thoughts with us. We genuinely appreciate your feedback, as it helps us identify areas where we can improve.\n",
      "\n",
      "We are sorry to hear that your experience did not meet your expectations. It’s concerning to us that [specific issue] impacted your experience negatively. We strive to provide each of our customers with the best possible service, and it’s clear that we fell short in this instance.\n",
      "\n",
      "We would love the opportunity to learn more about your experience and how we can make things right. If you're open to it, please reach out to us directly at [contact information]. Your insights are invaluable to us, and we want to ensure you feel heard and valued.\n",
      "\n",
      "Thank you once again for your feedback. We hope to have the chance to improve your experience in the future.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]  \n",
      "[Your Position]  \n",
      "[Company Name]  \n",
      "[Contact Information]\n"
     ]
    }
   ],
   "source": [
    "review = \"The product is terrible. It broke after just one use and quality is very poor\"\n",
    "result = chain.invoke({\"feedback\": review})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
