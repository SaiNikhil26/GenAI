{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Chat Models\n",
    "\n",
    "- Basic Conversations\n",
    "- Alternatives\n",
    "- Conversation with Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\") # load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to simply call the model and ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an AI language model designed to assist with information and creative tasks across a wide range of topics.\n"
     ]
    }
   ],
   "source": [
    "# invoke the model with a simple message\n",
    "result = model.invoke(\"Hey, tell me about yourself in a single line.\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of messages in LangChain, how to use `messages` list to store conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 + 5 = 10\n"
     ]
    }
   ],
   "source": [
    "# import various types of messages\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"), # message to guide the AI\n",
    "    HumanMessage(\"What is 2 + 2?\"), # message from the user\n",
    "    AIMessage(\"2 + 2 = 4\"), # output message from AI\n",
    "    HumanMessage(\"Now tell me what is 5 + 5\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages) # invoke the model with multiple messages / a conversation\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the LangChain API keeps the process unified and makes it easier to migrate from a single model to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# this is markdown code, not runnable\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model_gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-8b\")\n",
    "model_claude = ChatAnthropic(model=\"claude-3-sonnet-20240229\")\n",
    "\n",
    "result_openai = model.invoke(\"What is 5+5?\")\n",
    "print(\"ChatGPT:\",result_openai.content)\n",
    "\n",
    "result_gemini = model_gemini.invoke(\"What is 5+5?\")\n",
    "print(\"Gemini:\", result_gemini.content)\n",
    "\n",
    "result_claude = model_claude.invoke(\"What is 5+5?\")\n",
    "print(\"Claude:\", result_claude.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to have real-time conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hey im pranav\n",
      "AI: Hi Pranav! How can I assist you today?\n",
      "You: what is my name?\n",
      "AI: Your name is Pranav. How can I help you today?\n",
      "You: exit\n",
      "--- MESSAGE HISTORY ---\n",
      "content='You are a helpful assistant.' additional_kwargs={} response_metadata={}\n",
      "content='hey im pranav' additional_kwargs={} response_metadata={}\n",
      "content='Hi Pranav! How can I assist you today?' additional_kwargs={} response_metadata={}\n",
      "content='what is my name?' additional_kwargs={} response_metadata={}\n",
      "content='Your name is Pranav. How can I help you today?' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "chat_history = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"),\n",
    "]\n",
    "\n",
    "while True:\n",
    "    query=input()\n",
    "    print(\"You:\",query)\n",
    "    if query.lower() in (\"exit\",\"quit\"): break\n",
    "    chat_history.append(HumanMessage(query))\n",
    "\n",
    "    result = model.invoke(chat_history)\n",
    "    print(\"AI:\",result.content)\n",
    "    chat_history.append(AIMessage(result.content))\n",
    "\n",
    "print(\"--- MESSAGE HISTORY ---\")\n",
    "for msg in chat_history: print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `ConversationChain` to implement conversations and use different types of 'Conversation Memory' to add memory to the AI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pb19o\\AppData\\Local\\Temp\\ipykernel_6156\\1741181476.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory1 = ConversationBufferMemory() # simple buffer memory\n",
      "C:\\Users\\pb19o\\AppData\\Local\\Temp\\ipykernel_6156\\1741181476.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import *\n",
    "\n",
    "memory1 = ConversationBufferMemory() # simple buffer memory\n",
    "conversation = ConversationChain(\n",
    "    llm=model,\n",
    "    verbose=True,\n",
    "    memory=memory1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hey, good evening, how are you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hey, good evening, how are you?\n",
      "AI: Good evening! I'm doing well, thank you for asking. How about you? What’s on your mind tonight?\n",
      "Human: What are you good at? Tell me in one line\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm good at providing information, answering questions, and engaging in friendly conversations on a wide range of topics!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hey, good evening, how are you?\")\n",
    "conversation.predict(input=\"What are you good at? Tell me in one line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hey, good evening, how are you?\n",
      "AI: Good evening! I'm doing well, thank you for asking. How about you? What’s on your mind tonight?\n",
      "Human: What are you good at? Tell me in one line\n",
      "AI: I'm good at providing information, answering questions, and engaging in friendly conversations on a wide range of topics!\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, we can see that the messages are directly stored in a buffer memory.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few other memory classes - \n",
    "- `ConversationSummaryMemory` : Summarizes the conversation to save memory.\n",
    "- `ConversationBufferWindowMemory` : Just like buffer memory, with a `k=<count>` parameter to store last 'k' conversations.\n",
    "- `ConversationSummaryBufferMemory` : A hybrid memory which summarizes the conversation of each member. Has a `max_token_limit` parameter.\n",
    "\n",
    "```python\n",
    "# Try these out as well\n",
    "ConversationSummaryMemory(llm=model) \n",
    "\n",
    "ConversationBufferWindowMemory(llm=model,k=3) # last 3 msgs\n",
    "\n",
    "ConversationSummaryBufferMemory(llm=model, max_token_limit=100)\n",
    "\n",
    "```\n",
    "\n",
    "**A few advanced memory classes are - `EntityMemory` and `ConversationKnowledgeGraphMemory`**\n",
    "- \"EntityMemory\" class is used to make the LLM retain history in the form of entity/object context.\n",
    "\n",
    "For example, if you send \"John ate an apple\", it will understand that 'John' and 'apple' are entities.\n",
    "\n",
    "- \"ConversationKnowledgeGraphMemory\" class is used to make the LLM understand the context as well using knowledge graphs.\n",
    "\n",
    "For example, if you send \"John ate an apple\", then \"An apple is a fruit\", then ask \"What kind of food did John eat?\", then the LLM will understand it as - \n",
    "\"John -> [eats] -> Apple -> [is] -> Fruit\" \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prompt Templates\n",
    "\n",
    "- Basic Chat Prompt Template with template strings\n",
    "- Prompts with Multiple Placeholders\n",
    "- Prompts with System and Human Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI go broke? Because it couldn't find its cache!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Using template strings\n",
    "template_str = \"Tell me a good one-liner joke about {topic}\" # adding more variables makes the prompt use multiple placeholders\n",
    "prompt_template = ChatPromptTemplate.from_template(template_str)\n",
    "prompt = prompt_template.invoke(input={\"topic\":\"AI\"})\n",
    "result = model.invoke(prompt)\n",
    "print(result.content) # The joke probably won't be funny, but it works atleast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here’s a nature joke for you:\n",
      "\n",
      "Why did the tree go to therapy?\n",
      "\n",
      "Because it couldn't stop \"leafing\" its problems behind!\n"
     ]
    }
   ],
   "source": [
    "# using different type of messages\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"),\n",
    "    HumanMessage(\"Hello\"),\n",
    "    AIMessage(\"Hello, how can I help you?\"),\n",
    "    (\"human\",\"Tell me a joke about {topic}\") # use tuples if string interpolation is needed (for prompt variables)\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke(input={\"topic\":\"Nature\"})\n",
    "result = model.invoke(prompt)\n",
    "print(result.content) # once again, not funny, and yes again, it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chains\n",
    "\n",
    "- Basic Chains, learning to use Parsers\n",
    "- Parallel Chain Execution\n",
    "- Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nature is a breathtaking tapestry of life, showcasing the beauty and interconnectedness of all living things.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 40, 'total_tokens': 60, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-5d6fde96-6846-483f-bad5-5ad56e17f29a-0', usage_metadata={'input_tokens': 40, 'output_tokens': 20, 'total_tokens': 60, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser # helps display the output in a readable format\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"),\n",
    "    HumanMessage(\"Hello\"),\n",
    "    AIMessage(\"Hello, how can I help you?\"),\n",
    "    (\"human\",\"Tell me one line about {topic}\") # use tuples if string interpolation is needed (for prompt variables)\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# USE PIPE NOTATION TO CREATE CHAINS\n",
    "chain = prompt_template | model # yay, your very first chain! \n",
    "\n",
    "# you don't need to generate the prompt from the template, the chain does it for you... make sure you pass in the dict in \"invoke()\"\n",
    "result = chain.invoke(input={\"topic\":\"Nature\"}) \n",
    "result #unreadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nature is a breathtaking tapestry of life, showcasing the intricate balance and beauty of ecosystems that sustain our planet.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "result = chain.invoke(input={\"topic\":\"Nature\"}) \n",
    "result #readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RUNNABLES - The fundamentals of chains**\n",
    "\n",
    "- Everything revolves around \"Runnable\" in LangChain.\n",
    "- `Runnable` is the superclass of all Runnable classes, and `RunnableSequence` is a virtual chain of Runnables.\n",
    "- Every `Runnable` object has `invoke()` method, something we're already familiar with.\n",
    "- A sequence of runnables, will each have the 'invoke' function which sends the output as input to the next Runnable node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nature is a beautiful and intricate tapestry of life, showcasing the harmony and balance of the ecosystems that sustain our planet.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
    "\n",
    "# Let's take the previous nature example \n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"),\n",
    "    HumanMessage(\"Hello\"),\n",
    "    AIMessage(\"Hello, how can I help you?\"),\n",
    "    (\"human\",\"Tell me one line about {topic}\") # use tuples if string interpolation is needed (for prompt variables)\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "runnable_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
    "runnable_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
    "runnable_parser = RunnableLambda(lambda x: StrOutputParser().invoke(x))\n",
    "\n",
    "runnable_sequence = RunnableSequence(first = runnable_prompt, middle=[runnable_model], last=runnable_parser)\n",
    "result = runnable_sequence.invoke(input={\"topic\":\"Nature\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "# How to create longer chains? Nothing much, just add more runnables\n",
    "\n",
    "# Now we have a runnable sequence (which is a chain) that gives us a line on a given topic\n",
    "# Let's split the output into words, then count the number of words\n",
    "\n",
    "word_splitter = RunnableLambda(lambda x: x.split())\n",
    "word_counter = RunnableLambda(lambda x: len(x))\n",
    "\n",
    "final_chain = runnable_sequence | word_splitter | word_counter\n",
    "result = final_chain.invoke(input={\"topic\":\"Nature\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few other Runnables, which can be quite useful:\n",
    "\n",
    "- `RunnablePassthrough` : sends the entire input back as output (nothing changed)\n",
    "- `RunnablePick` : picks a specific key and returns its value from the input\n",
    "- `RunnableAssign` : assigns values for some of the prompt variables in the input\n",
    "\n",
    "Now we'll be looking at how to create parallel chains and branched chains!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel Chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pros: \n",
      "Based on the features outlined for the Samsung Galaxy M14 5G, here are 2-3 pros of the product:\n",
      "\n",
      "1. **Impressive Battery Life**: The 6000 mAh battery capacity is a standout feature, allowing users to enjoy extended usage without the need for frequent recharging. This is particularly beneficial for those who are heavy users or often on the go.\n",
      "\n",
      "2. **Strong Display Quality**: The 6.6-inch FHD+ display provides vibrant colors and decent viewing angles, making it ideal for media consumption, whether you're streaming videos or browsing social media.\n",
      "\n",
      "3. **5G Connectivity**: With support for 5G networks, the Galaxy M14 5G is a future-proof option for users looking to benefit from faster internet speeds, ensuring that the device remains relevant as network technologies evolve.\n",
      "\n",
      "Cons: \n",
      "While the Samsung Galaxy M14 5G has many strengths, there are a few cons to consider:\n",
      "\n",
      "1. **Camera Performance in Low Light**: Although the 50 MP main sensor performs well in good lighting conditions, its performance tends to drop in low-light situations. This limitation may be a drawback for users who frequently take photos in less-than-ideal lighting.\n",
      "\n",
      "2. **Lack of Premium Features**: As a budget smartphone, the M14 5G may not include some of the advanced features found in higher-end models, such as superior camera capabilities (e.g., optical zoom, advanced night modes) and faster charging options. Users looking for a more feature-rich experience might find this lacking.\n",
      "\n",
      "3. **Performance Limitations**: While the Exynos processor provides smooth performance for everyday tasks and light gaming, it may struggle with more demanding applications or intensive gaming. Users who need a phone for heavy multitasking or gaming may find the performance somewhat limiting.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnableBranch\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are an expert product reviewer.\"),\n",
    "    (\"human\",\"Give me a brief but general review about the product: {product}\")\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# write functions for the parallel chains\n",
    "\n",
    "def analyze_pros(features):\n",
    "    pros_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"You are an expert product reviewer.\"),\n",
    "        (\"human\",\"Given these features: {features}, what are 2-3 pros of the product?\")\n",
    "    ])\n",
    "    return pros_template.format_prompt(features=features)\n",
    "\n",
    "def analyze_cons(features):\n",
    "    cons_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"You are an expert product reviewer.\"),\n",
    "        (\"human\",\"Given these features: {features}, what are 2-3 cons of the product?\")\n",
    "    ])\n",
    "    return cons_template.format_prompt(features=features)\n",
    "\n",
    "def final_output(pros, cons): return f\"\\nPros: \\n{pros}\\n\\nCons: \\n{cons}\"\n",
    "\n",
    "# create the chains\n",
    "\n",
    "pros_branch = RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
    "cons_branch = RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
    "\n",
    "# NOTICE HOW A PARALLEL CHAIN IS CREATED (it is also a runnable, obviously)\n",
    "final_chain = (\n",
    "    prompt_template | model | StrOutputParser() | \n",
    "    RunnableParallel(branches={\"pros\":pros_branch,\"cons\":cons_branch}) | \n",
    "    RunnableLambda(lambda x: final_output(x[\"branches\"][\"pros\"],x[\"branches\"][\"cons\"]))\n",
    ")\n",
    "\n",
    "result = final_chain.invoke(input={\"product\": \"Samsung Galaxy M14 5G\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Branched Chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Thank You for Your Kind Words!\n",
      "\n",
      "Dear [Recipient's Name],\n",
      "\n",
      "Thank you so much for your positive feedback! We truly appreciate you taking the time to share your thoughts. It’s always wonderful to hear that our efforts are making a difference.\n",
      "\n",
      "Your support motivates us to continue striving for excellence. If you have any suggestions or further feedback, please don’t hesitate to reach out. We’re here to help!\n",
      "\n",
      "Thanks again, and have a fantastic day!\n",
      "\n",
      "Best regards,  \n",
      "[Your Name]  \n",
      "[Your Position]  \n",
      "[Your Company]  \n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"You are an expert in classifying customer feedbacks. You have to classify and just return the category name!\"),\n",
    "    (\"human\",\"Categorize this feedbacks: {feedback} - into one of the following categories: {categories}.\")\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "main_chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "categories = [\"positive\",\"negative\",\"neutral\"]\n",
    "\n",
    "categorical_prompt_templates = {\n",
    "    \"positive\": ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Generate a thank you response for this positive feedback: {feedback}\")\n",
    "    ]),\n",
    "    \"negative\": ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Generate a response addressing this negative feedback: {feedback}\")\n",
    "    ]),\n",
    "    \"neutral\": ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Generate a request response for this neutral feedback: {feedback}\")\n",
    "    ])\n",
    "}\n",
    "\n",
    "# create branches for different categories\n",
    "branches = RunnableBranch(\n",
    "    (\n",
    "        lambda x: \"positive\" in x,\n",
    "        categorical_prompt_templates[\"positive\"] | model | StrOutputParser()\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"negative\" in x,\n",
    "        categorical_prompt_templates[\"negative\"] | model | StrOutputParser()\n",
    "    ),\n",
    "    categorical_prompt_templates[\"neutral\"] | model | StrOutputParser() # don't forget this default case!\n",
    ")\n",
    "\n",
    "final_chain = main_chain | branches # create the final chain\n",
    "\n",
    "f = \"This was a great experience and I loved the product!\"\n",
    "result = final_chain.invoke({\"feedback\":f,\"categories\":categories})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RAG - Retrieval Augmented Generation \n",
    "\n",
    "- RAG Basics, Vector Stores, RAG Metadata\n",
    "- Text Splitting, Embedding\n",
    "- One-off Question\n",
    "- Conversational RAG\n",
    "- Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "file_path = r\"data/animals.txt\"\n",
    "persist_directory = r\"db/chroma_db\"\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# No need to create if already exists\n",
    "if not os.path.exists(persist_directory):\n",
    "    print(\"--- Initializing Vector Store ---\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found at {file_path}\")\n",
    "    \n",
    "    # initialize the text loader, load the documents, split them into chunks\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Number of chunks: {len(docs)}\")\n",
    "\n",
    "    # create embeddings, initialize the vector store\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)\n",
    "\n",
    "    print(\"Embeddings ready, vector store initialized.\")\n",
    "else: print(\"Vector store already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pb19o\\AppData\\Local\\Temp\\ipykernel_6156\\1459548579.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=persist_directory,embedding_function=embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant docs:\n",
      "\n",
      "Document: 1\n",
      "#### c) Reptiles  \n",
      "- Cold-blooded animals with scales or bony plates.  \n",
      "- Most lay eggs, while some give birth to live young.  \n",
      "- Examples: Snakes, Lizards, Turtles, and Crocodiles.  \n",
      "\n",
      "#### d) Amphibians  \n",
      "- Cold-blooded animals that live both in water and on land.  \n",
      "- Have moist skin that helps in respiration.  \n",
      "- Examples: Frogs, Salamanders, and Toads.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's perform RAG \n",
    "db = Chroma(persist_directory=persist_directory,embedding_function=embeddings)\n",
    "query = \"Tell me about 'amphibians'\"\n",
    "\n",
    "# we need a 'retriever' to perform the R in RAG\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\":1,\"score_threshold\":0.3} # give me the most relevant document\n",
    ")\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"Relevant docs:\\n\")\n",
    "for i,doc in enumerate(relevant_docs,1):\n",
    "    print(f\"Document: {i}\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is one single document chunk which contains content about \"amphibians\". \n",
    "(The similarity score is a bit lower than expected, but it does the job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store already exists.\n"
     ]
    }
   ],
   "source": [
    "# Now let's see why adding metadata to RAG is important\n",
    "\n",
    "data_folder = r\"data\"\n",
    "persist_directory = r\"db/chroma_db_with_metadata\"\n",
    "\n",
    "# No need to create if already exists\n",
    "if not os.path.exists(persist_directory):\n",
    "    print(\"--- Initializing Vector Store ---\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found at {file_path}\")\n",
    "    \n",
    "    files = [f for f in os.listdir(data_folder) if f.endswith(\".txt\")]\n",
    "    documents = []\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(data_folder,file)\n",
    "        loader = TextLoader(file_path)\n",
    "        text_docs = loader.load()\n",
    "        for doc in text_docs:\n",
    "            doc.metadata = {\"Source\":file} # add metadata for the document\n",
    "            documents.append(doc)\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Number of chunks: {len(docs)}\")\n",
    "\n",
    "    # create embeddings, initialize the vector store\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)\n",
    "\n",
    "    print(\"Embeddings ready, vector store initialized.\")\n",
    "else: print(\"Vector store already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant docs:\n",
      "\n",
      "Document: 1\n",
      "### **3. Rock and Pop**\n",
      "- Rock emerged in the 1950s, evolving into subgenres like punk, metal, and alternative rock.\n",
      "- Pop music is characterized by catchy melodies and broad appeal.\n",
      "- Icons: The Beatles, Michael Jackson, Madonna.\n",
      "\n",
      "### **4. Hip-Hop and Rap**\n",
      "- Originated in the 1970s as a voice for social issues and storytelling.\n",
      "- Features rhythmic beats, poetry, and rap lyrics.\n",
      "- Influencers: Tupac Shakur, The Notorious B.I.G., Kendrick Lamar.\n",
      "\n",
      "Source:  music.txt\n"
     ]
    }
   ],
   "source": [
    "db = Chroma(persist_directory=persist_directory,embedding_function=embeddings)\n",
    "query = \"Give me a two-liner on hip-hop music\"\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\":1,\"score_threshold\":0.2}\n",
    ")\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"Relevant docs:\\n\")\n",
    "for i,doc in enumerate(relevant_docs,1):\n",
    "    print(f\"Document: {i}\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(\"Source: \",doc.metadata[\"Source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see \"Source: music.txt\" - which means we know what document/source the model retrieves the data from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are many kinds of text splitters, which can be used for various occasions**\n",
    "\n",
    "- `CharacterTextSplitter` - The usual one, useful for consistent chunking regardless of context\n",
    "- `SentenceTransformersTokenTextSplitter` - Splitting sentences, to maintain semantic coherence\n",
    "- `TokenTextSplitter` - Used when there are strict token limits\n",
    "- `RecursiveCharacterTextSplitter` - Balances between maintaining semantic coherence and adhering to token limits\n",
    "\n",
    "and this is how you can use the `TextSplitter` superclass to create a custom text splitter - \n",
    "\n",
    "```python\n",
    "class MyTextSplitter(TextSplitter):\n",
    "    def split_text(self, text):\n",
    "        # text split logic\n",
    "        return text.split(\"\\n\\n\") # split by paragraphs\n",
    "```\n",
    "\n",
    "Similarly, even for Embeddings, we can use different OpenAI embedding models, or even use `HuggingFaceEmbeddings` and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now let's see how to get a proper response from the model, like having a conversation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant docs:\n",
      "\n",
      "Document: 1\n",
      "### **3. Rock and Pop**\n",
      "- Rock emerged in the 1950s, evolving into subgenres like punk, metal, and alternative rock.\n",
      "- Pop music is characterized by catchy melodies and broad appeal.\n",
      "- Icons: The Beatles, Michael Jackson, Madonna.\n",
      "\n",
      "### **4. Hip-Hop and Rap**\n",
      "- Originated in the 1970s as a voice for social issues and storytelling.\n",
      "- Features rhythmic beats, poetry, and rap lyrics.\n",
      "- Influencers: Tupac Shakur, The Notorious B.I.G., Kendrick Lamar.\n",
      "\n",
      "Source:  music.txt\n",
      "\n",
      "\n",
      " Hip-hop originated in the 1970s as a voice for social issues and storytelling, featuring rhythmic beats and rap lyrics. Influencers include Tupac Shakur, The Notorious B.I.G., and Kendrick Lamar.\n"
     ]
    }
   ],
   "source": [
    "db = Chroma(persist_directory=persist_directory,embedding_function=embeddings)\n",
    "query = \"Give me a two-liner on hip-hop music\"\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\":1,\"score_threshold\":0.2}\n",
    ")\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"Relevant docs:\\n\")\n",
    "for i,doc in enumerate(relevant_docs,1):\n",
    "    print(f\"Document: {i}\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(\"Source: \",doc.metadata[\"Source\"])\n",
    "\n",
    "combined_input = (\n",
    "    f\"\"\"\n",
    "    Here are some documents that will help you answer the user queries:\n",
    "\n",
    "    Documents:\n",
    "    {\"\\n\\n\".join([doc.page_content for doc in relevant_docs])}\n",
    "\n",
    "    Provide answer only based on the documents provided. If the answer is not available, say that you don't know. Don't generate answers.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\"system\",combined_input),\n",
    "    (\"human\",query)\n",
    "]\n",
    "\n",
    "result = model.invoke(messages) # you can create a prompt template, add parsers, chain it together, blah blah blah.. but this is enough\n",
    "print(\"\\n\\n\",result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was basically \"one-off question\" - like basically querying one by one. Now let's have proper conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":1}\n",
    ") # we're using the same \"chroma_db_with_metadata\" vector store db\n",
    "\n",
    "context_prompt = \"\"\"\n",
    "    Use the chat history to contextualize the user prompt as a standalone question WITHOUT ANSWERING THE QUESTION!\n",
    "\"\"\"\n",
    "\n",
    "context_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",context_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\",\"{input}\") # usually can be 'user_input' or 'user_query', but has to be 'input' here for history_aware_retriever\n",
    "])\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    Use the given context to answer the user query. If the answer is not available, say that you don't know.\n",
    "    {context}\n",
    "\n",
    "    Keep the answer short and simple.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\",\"{input}\") # usually can be 'user_input' or 'user_query', but has to be 'input' here for history_aware_retriever\n",
    "])\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(model, retriever, context_prompt_template)\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(model, prompt_template)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting! Type 'exit'/'quit' to end the conversation.\n",
      "\n",
      "\n",
      "You: tell me about reptiles\n",
      "AI: Reptiles are cold-blooded animals with scales or bony plates. Most of them lay eggs, although some give birth to live young. Examples include snakes, lizards, turtles, and crocodiles.\n",
      "You: what class do they belong to\n",
      "AI: Reptiles belong to the class Reptilia.\n",
      "You: exit\n"
     ]
    }
   ],
   "source": [
    "def start_conversation():\n",
    "    print(\"Start chatting! Type 'exit'/'quit' to end the conversation.\\n\\n\")\n",
    "    \n",
    "    chat_history = []\n",
    "\n",
    "    while True:\n",
    "        query  = input()\n",
    "        print(f\"You: {query}\")\n",
    "        if query.lower() in (\"exit\",\"quit\"): break\n",
    "        result = rag_chain.invoke({\n",
    "            \"input\":query,\n",
    "            \"chat_history\":chat_history\n",
    "        })\n",
    "        print(f\"AI: {result['answer']}\")\n",
    "\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(AIMessage(content=result['answer']))\n",
    "\n",
    "start_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Web-Scraping RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store already exists\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "\n",
    "def create_vector_store():\n",
    "    \"\"\"Use web content to create a vector store\"\"\"\n",
    "\n",
    "    persist_directory = r\"db/chroma_db_web\"\n",
    "\n",
    "    if not os.path.exists(persist_directory):\n",
    "        print(\"--- Initializing Vector Store ---\")\n",
    "        # don't use any private/personal/protected sites here - I've used a public sandbox site!!\n",
    "        loader = FireCrawlLoader(url=\"https://www.scrapethissite.com/pages/simple/\", mode=\"scrape\")\n",
    "        documents = loader.load()\n",
    "\n",
    "        print(f\"Number of documents: {len(documents)}\")\n",
    "\n",
    "        # Convert metadata into strings (web metadata can be lists)\n",
    "        for doc in documents:\n",
    "            for k,v in doc.metadata.items():\n",
    "                if isinstance(v, list):\n",
    "                    doc.metadata[k] = \", \".join(map(str,v))\n",
    "\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=0)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        print(f\"Number of document chunks: {len(docs)}\")\n",
    "\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)\n",
    "\n",
    "        print(\"Embeddings ready, vector store initialized.\")\n",
    "    else: print(\"Vector store already exists\")\n",
    "\n",
    "create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant docs:\n",
      "\n",
      "Document: 1\n",
      "**Capital:** Douglas\n",
      "\n",
      "**Population:** 75049\n",
      "\n",
      "**Area (km2):** 572.0\n",
      "\n",
      "### India\n",
      "\n",
      "**Capital:** New Delhi\n",
      "\n",
      "**Population:** 1173108018\n",
      "\n",
      "**Area (km2):** 3287590.0\n",
      "\n",
      "### British Indian Ocean Territory\n",
      "\n",
      "**Capital:** None\n",
      "\n",
      "**Population:** 4000\n",
      "\n",
      "**Area (km2):** 60.0\n",
      "\n",
      "### Iraq\n",
      "\n",
      "**Capital:** Baghdad\n",
      "\n",
      "**Population:** 29671605\n",
      "\n",
      "**Area (km2):** 437072.0\n",
      "\n",
      "### Iran\n",
      "\n",
      "**Capital:** Tehran\n",
      "\n",
      "**Population:** 76923300\n",
      "\n",
      "**Area (km2):** 1648000.0\n",
      "\n",
      "### Iceland\n",
      "\n",
      "**Capital:** Reykjavik\n",
      "\n",
      "**Population:** 308910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query_vector_store(query):\n",
    "    db = Chroma(persist_directory=r\"db/chroma_db_web\",embedding_function=embeddings)\n",
    "    retriever = db.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\":1}\n",
    "    )\n",
    "\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "\n",
    "    print(\"Relevant docs:\\n\")\n",
    "    for i,doc in enumerate(relevant_docs,1):\n",
    "        print(f\"Document: {i}\\n{doc.page_content}\\n\")\n",
    "\n",
    "query_vector_store(\"What is the population of India?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see how the correct relevant doc has been retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Agents and Tools\n",
    "\n",
    "- Agent and Tool Basics\n",
    "- ReAct Prompts\n",
    "- Tool Decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agent Basics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_structured_chat_agent, create_tool_calling_agent\n",
    "from langchain_core.tools import StructuredTool, tool\n",
    "from pydantic import BaseModel\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pb19o\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create a function that can be used as a tool\n",
    "@tool\n",
    "def get_current_time():\n",
    "    \"\"\"Returns current time in HH:MM:SS format\"\"\" # docstrings needed\n",
    "    from datetime import datetime\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "# set list of tools\n",
    "tools = [get_current_time]\n",
    "\n",
    "# react prompt\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# let's create our react agent\n",
    "agent = create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    "    stop_sequence=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out the current time.  \n",
      "Action: get_current_time  \n",
      "Action Input: None  \u001b[0m\u001b[36;1m\u001b[1;3m19:32:13\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer  \n",
      "Final Answer: The current time is 19:32:13.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What is the current time?', 'output': 'The current time is 19:32:13.'}\n"
     ]
    }
   ],
   "source": [
    "# we need an agent executor to run the agent, combine with tools\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = agent_executor.invoke({\"input\":\"What is the current time?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know how to simply create an agent, add a tool to it and use \"ReAct Prompting\" to invoke the model and get response.\n",
    "\n",
    "Next we'll add a few more tools and create a ReAct Chat conversation agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pb19o\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# NOTE: the @tool decorator turns the function into a 'StructuredTool' object\n",
    "\n",
    "# let's create a class for our arguments (args_schema) and create our own StructuredTools\n",
    "class DefaultQuerySchema(BaseModel):\n",
    "    query: str\n",
    "\n",
    "def search_wikipedia(query):\n",
    "    \"\"\"Returns content from wikipedia\"\"\" \n",
    "    from wikipedia import summary\n",
    "    try: return summary(query, sentences=2)\n",
    "    except: return \"Couldn't find any information on that.\"\n",
    "\n",
    "search_wiki_tool = StructuredTool.from_function(\n",
    "    name=\"Wiki\",\n",
    "    func=search_wikipedia,\n",
    "    description=\"Returns Wikipedia content\",\n",
    "    args_schema=DefaultQuerySchema\n",
    ")\n",
    "\n",
    "def calculate_expression(query):\n",
    "    \"\"\"Calculates the expression\"\"\"\n",
    "    return str(eval(query))\n",
    "\n",
    "calculate_tool = StructuredTool.from_function(\n",
    "    name=\"Calc\",\n",
    "    func=calculate_expression,\n",
    "    description=\"Returns evaluated result\",\n",
    "    args_schema=DefaultQuerySchema\n",
    ")\n",
    "\n",
    "tools = [get_current_time, search_wiki_tool, calculate_tool]\n",
    "\n",
    "# react prompt for chat agent\n",
    "prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
    "\n",
    "# we need chat history memory for the agent\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# let's create our react chat agent\n",
    "agent = create_structured_chat_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "initial_system_message = \"\"\"\n",
    "    You are a helpful AI assistant. Use ONLY the tools to provide answers. \n",
    "    For tools that require a query input, provide ONLY a string for the 'query' parameter.\n",
    "    Do not use dictionaries with 'title' or other keys as input.\n",
    "    Run the tool not more than thrice to decide on an output.\n",
    "    If the tools are not capable of providing necessary responses, then say that you can't provide a response.\n",
    "\"\"\"\n",
    "memory.chat_memory.add_message(SystemMessage(content=initial_system_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: one line about john cena\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "{\n",
      "  \"action\": \"Wiki\",\n",
      "  \"action_input\": \"John Cena\"\n",
      "}\n",
      "```\u001b[0m\u001b[33;1m\u001b[1;3mCouldn't find any information on that.\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"John Cena is an American professional wrestler, actor, and television presenter known for his time in WWE.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: John Cena is an American professional wrestler, actor, and television presenter known for his time in WWE.\n",
      "You: give me the exact current time\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "{\n",
      "  \"action\": \"get_current_time\",\n",
      "  \"action_input\": {}\n",
      "}\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3m19:32:59\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The current time is 19:32:59.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: The current time is 19:32:59.\n"
     ]
    }
   ],
   "source": [
    "# Begin chatting!\n",
    "while True:\n",
    "    user_input = input()\n",
    "    if user_input.lower() in (\"exit\",\"quit\"): break\n",
    "    print(f\"You: {user_input}\")\n",
    "    memory.chat_memory.add_message(HumanMessage(content=user_input))\n",
    "    response = agent_executor.invoke({\"input\":user_input})\n",
    "    print(f\"AI: {response[\"output\"]}\")\n",
    "    memory.chat_memory.add_message(AIMessage(content=response[\"output\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it works! Now let's start combining our learnings...\n",
    "Let's now create a \"RAG\" \"Agent\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a RAG Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pb19o\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# for the vector store, we'll be using the \"chroma_db_with_metadata\" which we already have\n",
    "db = Chroma(persist_directory=r\"db/chroma_db_with_metadata\", embedding_function=embeddings)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")\n",
    "\n",
    "# we'll also use the same context and system prompts, history aware retrievers, etc.\n",
    "context_prompt = \"\"\"\n",
    "    Use the chat history to contextualize the user prompt as a standalone question WITHOUT ANSWERING THE QUESTION!\n",
    "\"\"\"\n",
    "\n",
    "context_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",context_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\",\"{input}\") # usually can be 'user_input' or 'user_query', but has to be 'input' here for history_aware_retriever\n",
    "])\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    Use the given context to answer the user query. If the answer is not available, say that you don't know.\n",
    "    {context}\n",
    "\n",
    "    Keep the answer short and simple.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\",\"{input}\") # usually can be 'user_input' or 'user_query', but has to be 'input' here for history_aware_retriever\n",
    "])\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(model, retriever, context_prompt_template)\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(model, prompt_template)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
    "\n",
    "# this will be a ReAct RAG Agent, so let's get our prompt\n",
    "react_prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create the functions and tools necessary\n",
    "\n",
    "qafn = lambda input, **kwargs: rag_chain.invoke({\n",
    "    \"input\": input,\n",
    "    \"chat_history\": kwargs.get(\"chat_history\",[])\n",
    "})\n",
    "\n",
    "qa_tool = StructuredTool.from_function(\n",
    "    name=\"QA\",\n",
    "    func=qafn,\n",
    "    description=\"Useful when questions have to be answered only from the given context\",\n",
    "    args_schema=DefaultQuerySchema # the same args_schema as before\n",
    ")\n",
    "\n",
    "tools = [qa_tool]\n",
    "\n",
    "rag_agent = create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=react_prompt\n",
    ")\n",
    "\n",
    "rag_agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=rag_agent,\n",
    "    tools=tools,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: tell me one line about reptiles\n",
      "AI: Reptiles are cold-blooded animals with scales or bony plates, most of which lay eggs.\n",
      "You: what class do birds belong to?\n",
      "AI: Birds belong to the class Aves.\n",
      "You: what is the recent film that was released?\n",
      "AI: I don't know.\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "# Let's now talk with the RAG Agent\n",
    "\n",
    "chat_history = []\n",
    "while True:\n",
    "    query = input()\n",
    "    print(f\"You: {query}\")\n",
    "    if query.lower() in (\"exit\",\"quit\"): break\n",
    "    response = rag_agent_executor.invoke({\"input\":query,\"chat_history\":chat_history})\n",
    "    print(f\"AI: {response['output']}\")\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    chat_history.append(AIMessage(content=response['output']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an agent that performs RAG tasks for us! That's nice. Finally, we'll see how \"tool calling agents\" are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pb19o\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We'll use the same 2 tools we had created already - the current time one, and the calculator one\n",
    "\n",
    "tools = [get_current_time, calculate_tool]\n",
    "tc_prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "tc_agent = create_tool_calling_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=tc_prompt\n",
    ")\n",
    "\n",
    "tc_agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=tc_agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: what is 10+20\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Calc` with `{'query': '10+20'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m30\u001b[0m\u001b[32;1m\u001b[1;3mThe result of \\( 10 + 20 \\) is \\( 30 \\).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: The result of \\( 10 + 20 \\) is \\( 30 \\).\n",
      "You: what is the time right now?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_current_time` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m20:00:00\u001b[0m\u001b[32;1m\u001b[1;3mThe current time is 20:00:00.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: The current time is 20:00:00.\n",
      "You: exit\n"
     ]
    }
   ],
   "source": [
    "# Let's test it out now! \n",
    "while True:\n",
    "    query = input()\n",
    "    print(f\"You: {query}\")\n",
    "    if query.lower() in (\"exit\",\"quit\"): break\n",
    "    response = tc_agent_executor.invoke({\"input\":query})\n",
    "    print(f\"AI: {str(response['output'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
